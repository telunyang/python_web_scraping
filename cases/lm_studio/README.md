# lm studio
- åœ¨ localhost å®‰è£ large language model (LLM) ä¼ºæœå™¨ï¼Œä¸¦ä½¿ç”¨ OpenAI çš„ API ä¾†é€²è¡Œå°è©±ã€åœ–ç‰‡æè¿°ç­‰æ‡‰ç”¨ã€‚
  - [é¦–é ](https://lmstudio.ai/)
    - è«‹ä¾ä½œæ¥­ç³»çµ±ä¸‹è¼‰é©åˆçš„ç‰ˆæœ¬
  - [ä½¿ç”¨èªªæ˜](https://lmstudio.ai/docs/welcome)
- å¯ä»¥åƒè€ƒ [The Walking Fishçš„ç¨‹å¼å°ç«™](https://the-walking-fish.com/p/lmstudio/) æœ‰é—œ LM Studio çš„ä»‹ç´¹
- ä¸‹è¼‰çš„ LLM æ˜¯ [é‡åŒ–](https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c) å¾Œçš„ç‰ˆæœ¬
  - GGUF (GPT-Generated Unified Format) çš„ç›¸é—œèªªæ˜ï¼š
    - [GGUF](https://huggingface.co/docs/hub/en/gguf)
    - [TheBloke/Llama-2-13B-GGML](https://huggingface.co/TheBloke/Llama-2-13B-GGML)
	- [[Day 15] - é‹¼éµè‰æ³¥é¦¬ ğŸ¦™ LLM chatbot ğŸ¤– (6/10)ï½œGGML é‡åŒ– LLaMa](https://ithelp.ithome.com.tw/articles/10331431)
- index.html å’Œ web_api.py æ˜¯æ¸¬è©¦æ–‡å­—ä¸²æµæ•ˆæœçš„ç¯„ä¾‹ï¼Œåƒè€ƒé€£çµå¦‚ä¸‹ï¼š
  - [YouTube video](https://www.youtube.com/watch?v=z6iYcqNECwA)
  - [Source code](https://github.com/PrettyPrinted/youtube_video_code/tree/master/2024/03/28/How%20to%20Stream%20OpenAI%20API%20Responses%20in%20a%20Flask%20App/flask_openai_streaming)


## å¥—ä»¶å®‰è£
```bash
pip install openai requests flask
```

## æ¨¡å‹ä½¿ç”¨ç¯„ä¾‹
- è‹±æ–‡
  - ç¨‹å¼ç¢¼ç”Ÿæˆ
    - TheBloke/CodeLlama-7B-Instruct-GGUF/codellama-7b-instruct.Q8_0.gguf
  - å°è©±ç”Ÿæˆ
    - lmstudio-ai/gemma-2b-it-GGUF/gemma-2b-it-q8_0.gguf
- ç¹é«”ä¸­æ–‡ (Optional)
  - å°è©±ç”Ÿæˆ
    - audreyt/Taiwan-LLM-7B-v2.0.1-chat-GGUF/Taiwan-LLM-7B-v2.0.1-chat-Q5_K_M.gguf
    - audreyt/Taiwan-LLM-7B-v2.0.1-chat-GGUF/Taiwan-LLM-7B-v2.0.1-chat-Q8_0.gguf

## ä¸‹è¼‰æ¨¡å‹æµç¨‹
- é¸æ“‡æ¨¡å‹ï¼Œæœ€å¾ŒæŒ‰ä¸‹ Download
![](https://i.imgur.com/PlDyc8s.png)
- è§€çœ‹æ¨¡å‹ä¸‹è¼‰é€²åº¦
![](https://i.imgur.com/g9QEea6.png)
- æ¨¡å‹é©—è­‰ä¸­
![](https://i.imgur.com/xN5yyyI.png)
- é©—è­‰å®Œæˆ
![](https://i.imgur.com/vqBxafd.png)
- èˆ‡ AI å°è©± (éœ€è¦å…ˆé¸æ“‡æ¨¡å‹ï¼Œè®€å–éœ€è¦ä¸€æ®µæ™‚é–“)
![](https://i.imgur.com/fWTnQxz.png)

## æç¤ºå­—
ç³»çµ±æç¤º (system prompt)
```log
A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
```

ä½¿ç”¨è€…æç¤º (user prompt) è¨»: ä½¿ç”¨ CodeLlama-7B-Instruct-GGUF æ¨¡å‹
```bash
è«‹å¹«æˆ‘æ’°å¯«ä¸€æ®µ 9x9 ä¹˜æ³•è¡¨çš„ç¨‹å¼ç¢¼ï¼Œä½¿ç”¨ python ä¾†æ’°å¯«ï¼Œåªè¦ç¨‹å¼ç¢¼ç¯„ä¾‹å°±å¥½ï¼Œä¸ç”¨é¡å¤–èªªæ˜ã€‚
```
![](https://i.imgur.com/bIz47ui.png)

## ä½¿ç”¨è€…æç¤ºå­—ç¯„ä¾‹
- ä¸€èˆ¬ä½¿ç”¨è€…æç¤º
```
å•é¡Œï¼šæ˜¯ä½æ–¼è‡ºç£è‡ºåŒ—å¸‚ä¸­å±±å€åŠæ½­å±±çš„æ—…é¤¨ï¼Œæˆç«‹æ–¼ç¬¬äºŒæ¬¡ä¸–ç•Œå¤§æˆ°å¾Œï¼Œæ—©å¹´ç‚ºè‡ºç£é¦–å±ˆä¸€æŒ‡çš„å¤§å‹åœ‹éš›æ€§é£¯åº—ã€‚ç›®å‰æ‰€è¦‹çš„ä¸­åœ‹å®®æ®¿é¢¨æ ¼å»ºç¯‰æ–¼1973å¹´è½æˆï¼Œæ˜¯è‡ºåŒ—åœ°æ¨™ä¹‹ä¸€ã€‚é£¯åº—å±‹é¡¶ä½¿ç”¨æ­‡å±±å¼ã€‚é£¯åº—çš„å»ºç¯‰ä¸Šæ¡ç”¨ç›¸ç•¶å¤šçš„é¾å½¢é›•åˆ»ï¼Œæ•…æœ‰äººç¨±æ­¤é£¯åº—ç‚ºã€Œé¾å®®ã€ï¼› é™¤æ¡ç”¨é¾å½¢ä¹‹å¤–ï¼Œäº¦æœ‰çŸ³ç…ã€æ¢…èŠ±ç­‰ä¸­åœ‹å»ºç¯‰å¸¸ç”¨çš„åœ–æ¡ˆã€‚è©²å»ºç¯‰çš„åç¨±æ˜¯ï¼š(A)å¸åå¤§é£¯åº—(B)å°åŒ—åŸå¤§é£¯åº—(C)åœ“å±±å¤§é£¯åº—(D)å¤©æˆæ–‡æ—…è¯å±±ç”º

æ³¨æ„ï¼šä¸ç”¨éå¤šçš„èªªæ˜ï¼Œåªè¦æ­£ç¢ºåœ°å›ç­” (A),(B),(C),(D) å…¶ä¸­ä¸€å€‹å°±å¯ä»¥äº†ã€‚

ç­”æ¡ˆæ˜¯ï¼š
```
![](https://i.imgur.com/bLoK8sP.png)


- ä¸€èˆ¬ä½¿ç”¨è€…æç¤ºï¼šåŠ å…¥å•é¡Œç›¸é—œçš„èƒŒæ™¯çŸ¥è­˜ï¼Œå¹«åŠ© LLM å›ç­”å•é¡Œï¼Œä¾‹å¦‚é€éç¶²è·¯çˆ¬èŸ²ã€ç¶²é æœå°‹ã€è³‡æ–™åº«æŸ¥è©¢ç­‰æ–¹å¼å–å¾—èƒŒæ™¯çŸ¥è­˜
```log
èƒŒæ™¯çŸ¥è­˜å¦‚ä¸‹ï¼š
åœ“å±±å¤§é£¯åº—ï¼Œæ˜¯ä¸€åº§ä½æ–¼è‡ºç£è‡ºåŒ—å¸‚ä¸­å±±å€åŠæ½­å±±çš„æ—…é¤¨ï¼Œæˆç«‹æ–¼ç¬¬äºŒæ¬¡ä¸–ç•Œå¤§æˆ°å¾Œã€‚é…’åº—å§‹å»ºæ–¼1952å¹´5æœˆï¼Œä¸»æ¨“è½æˆæ–¼1973å¹´10æœˆ10æ—¥ã€‚æ›¾ç‚º20ä¸–ç´€å¾ŒæœŸè‡ºç£é¦–å±ˆä¸€æŒ‡çš„å¤§å‹åœ‹éš›æ€§é£¯åº—ï¼Œä¸¦è‡ªå•Ÿç”¨å¾Œæ›¾æ¥å¾…éè¨±å¤šä¾†è‡ºåŒ—è¨ªå•çš„å¤–åœ‹æ”¿è¦ã€‚ç•¶å‰è‡ºåŒ—åœ“å±±å¤§é£¯åº—å·²æ™‰å‡ç‚ºäº¤é€šéƒ¨è§€å…‰å±€è©•é‘‘ä¹‹äº”æ˜Ÿç´šé£¯åº—ã€‚

å•é¡Œï¼šæ˜¯ä½æ–¼è‡ºç£è‡ºåŒ—å¸‚ä¸­å±±å€åŠæ½­å±±çš„æ—…é¤¨ï¼Œæˆç«‹æ–¼ç¬¬äºŒæ¬¡ä¸–ç•Œå¤§æˆ°å¾Œï¼Œæ—©å¹´ç‚ºè‡ºç£é¦–å±ˆä¸€æŒ‡çš„å¤§å‹åœ‹éš›æ€§é£¯åº—ã€‚ç›®å‰æ‰€è¦‹çš„ä¸­åœ‹å®®æ®¿é¢¨æ ¼å»ºç¯‰æ–¼1973å¹´è½æˆï¼Œæ˜¯è‡ºåŒ—åœ°æ¨™ä¹‹ä¸€ã€‚é£¯åº—å±‹é¡¶ä½¿ç”¨æ­‡å±±å¼ã€‚é£¯åº—çš„å»ºç¯‰ä¸Šæ¡ç”¨ç›¸ç•¶å¤šçš„é¾å½¢é›•åˆ»ï¼Œæ•…æœ‰äººç¨±æ­¤é£¯åº—ç‚ºã€Œé¾å®®ã€ï¼› é™¤æ¡ç”¨é¾å½¢ä¹‹å¤–ï¼Œäº¦æœ‰çŸ³ç…ã€æ¢…èŠ±ç­‰ä¸­åœ‹å»ºç¯‰å¸¸ç”¨çš„åœ–æ¡ˆã€‚è©²å»ºç¯‰çš„åç¨±æ˜¯ï¼š(A)å¸åå¤§é£¯åº—(B)å°åŒ—åŸå¤§é£¯åº—(C)åœ“å±±å¤§é£¯åº—(D)å¤©æˆæ–‡æ—…è¯å±±ç”º

è«‹å¾ä»¥ä¸Šé¸é …ä¸­é¸æ“‡ä¸¦å›ç­”ã€‚
æ³¨æ„ï¼š
1. ç­”æ¡ˆåªèƒ½æ˜¯é¸é … (A), (B), (C), (D) å…¶ä¸­ 1 å€‹ï¼Œå¿…é ˆåœ¨å›ç­”çš„é–‹é ­é¡¯ç¤ºç­”æ¡ˆï¼Œè€Œä¸”ç­”æ¡ˆå·¦é‚Šã€å³é‚Šéƒ½è¦æœ‰æ‹¬è™Ÿ(Parentheses)ã€‚
2. å›ç­”çš„å…§å®¹åªèƒ½æ˜¯ä¸€å¥è©±ï¼Œä¸èƒ½æœ‰æ›è¡Œç¬¦è™Ÿ(\n)ã€‚

ç­”æ¡ˆæ˜¯ï¼š
```
![](https://i.imgur.com/pl8EhvL.png)


- ä¸€èˆ¬ä½¿ç”¨è€…æç¤ºï¼šå¤šä¸€é»èƒŒæ™¯çŸ¥è­˜çš„ä½¿ç”¨è€…æç¤º - è¦æ³¨æ„æ”¯æ´ æœ€å¤§åºåˆ—é•·åº¦ (max_seq_length) æˆ– ä¸Šä¸‹æ–‡é•·åº¦ (context length) æ˜¯å¤šå°‘
```log
èƒŒæ™¯çŸ¥è­˜å¦‚ä¸‹ï¼š
æ—æ‡·æ°‘ï¼ˆï¼‰ï¼Œæ˜¯ä¸€åè‡ºç£ç·¨èˆå®¶åŠä½œå®¶ï¼Œç‚ºç¾ä»£èˆåœ˜é›²é–€èˆé›†å‰µè¾¦äººèˆ‡è—è¡“ç¸½ç›£ã€‚æ—æ‡·æ°‘ç‚ºåœ‹ç«‹æ”¿æ²»å¤§å­¸æ–°èå­¸å£«ã€æ„›è·è¯å¤§å­¸è—è¡“ç¢©å£«ã€‚2006å¹´ç²é¸ç‚ºDiscoveryé »é“ã€Šè‡ºç£äººç‰©èªŒã€‹çš„6åä¸»è§’ä¹‹1ã€‚
é›²é–€èˆé›†ï¼Œæ˜¯ä¸€å€‹è‡ºç£çš„ç¾ä»£èˆè¹ˆè¡¨æ¼”åœ˜é«”ï¼Œæ–¼1973å¹´ç”±æ—æ‡·æ°‘å‰µè¾¦ï¼Œæ˜¯è‡ºç£çš„ç¬¬ä¸€å€‹ç¾ä»£èˆè·æ¥­èˆåœ˜ã€‚é›²é–€ä¹‹åä¾†è‡ªã€Šå‘‚æ°æ˜¥ç§‹ã€‹ä¸­çš„ä¸€å¥è©±ï¼šã€Œé»ƒå¸æ™‚ï¼Œå¤§å®¹ä½œé›²é–€ï¼Œå¤§å·â€¦â€¦ã€ï¼Œä¹Ÿå°±æ˜¯å‚³èªªä¸­é»ƒå¸æ™‚ä»£èˆè¹ˆçš„åç¨±ã€‚é›²é–€èˆé›†æ›¾æ¨å‡ºå¤šå€‹èˆè¹ˆä½œå“ï¼Œç•¶ä¸­åŒ…æ‹¬æœ‰è–ªå‚³ï¼Œä¹æ­Œï¼Œå®¶æ—åˆå”±ï¼Œæµæµªè€…ä¹‹æ­Œï¼Œæ°´æœˆï¼Œç«¹å¤¢ï¼Œè¡Œè‰ç­‰ç­‰ã€‚

å•é¡Œï¼šè‡ºç£ç¬¬ä¸€å€‹ç¾ä»£èˆåŠ‡åœ˜ã€Œé›²é–€èˆé›†ã€çš„å‰µè¾¦äººèˆ‡è—è¡“ç¸½ç›£ã€‚2006å¹´ç²é¸ç‚ºDiscoveryé »é“ã€Šå°ç£äººç‰©èªŒã€‹çš„6åä¸»è§’ä¹‹1ã€‚ä»–çš„åå­—æ˜¯ï¼š(A)æ—æ‡·æ°‘(B)ç¾…æ–‡è£•(C)æåœ‹ä¿®(D)æ—æ€¡å›

è«‹å¾ä»¥ä¸Šé¸é …ä¸­é¸æ“‡ä¸¦å›ç­”ã€‚
æ³¨æ„ï¼š
1. ç­”æ¡ˆåªèƒ½æ˜¯é¸é …Â (A),Â (B),Â (C),Â (D)Â å…¶ä¸­ 1 å€‹ï¼Œå¿…é ˆåœ¨å›ç­”çš„é–‹é ­é¡¯ç¤ºç­”æ¡ˆï¼Œè€Œä¸”ç­”æ¡ˆå·¦é‚Šã€å³é‚Šéƒ½è¦æœ‰æ‹¬è™Ÿ(Parentheses)ã€‚
2. å›ç­”çš„å…§å®¹åªèƒ½æ˜¯ä¸€å¥è©±ï¼Œä¸èƒ½æœ‰æ›è¡Œç¬¦è™Ÿ(\n)ã€‚

ç­”æ¡ˆæ˜¯ï¼š
```
![](https://i.imgur.com/uCsXoCP.png)


## åœ¨ localhost å»ºç«‹ server
- é–‹å•Ÿ Local Inference Server (å®ƒæœƒé¡å¤–æä¾› Web API æœå‹™)
![](https://i.imgur.com/KAzGvMN.png)
- åŸ·è¡Œ web_api.py: `python web_api.py` (å®ƒæœƒä¸²æ¥ Local Inference Server çš„ Web API)
- é€²å…¥ [http://127.0.0.1:5000/](http://127.0.0.1:5000/)
- äº’å‹•å¾Œï¼Œæª¢è¦– Local Inference Server èˆ‡ç€è¦½å™¨é é¢
![](https://i.imgur.com/257lhXU.png)


## ç¯„ä¾‹ç¨‹å¼
**ä½¿ç”¨ CURL ä¾†å° local server é€²è¡ŒèŠå¤©å°è©±**
```bash
curl http://localhost:1234/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{ 
  "model": "TheBloke/CodeLlama-7B-Instruct-GGUF/codellama-7b-instruct.Q8_0.gguf",
  "messages": [ 
    { "role": "system", "content": "Always answer in rhymes." },
    { "role": "user", "content": "Introduce yourself." }
  ], 
  "temperature": 0.7, 
  "max_tokens": -1,
  "stream": true
}'
```

**ä½¿ç”¨ python ä¾†å° local server é€²è¡ŒèŠå¤©å°è©±**
```python
# Example: reuse your existing OpenAI setup
from openai import OpenAI

# Point to the local server
client = OpenAI(base_url="http://localhost:1234/v1", api_key="lm-studio")

completion = client.chat.completions.create(
  model="TheBloke/CodeLlama-7B-Instruct-GGUF/codellama-7b-instruct.Q8_0.gguf",
  messages=[
    {"role": "system", "content": "Always answer in rhymes."},
    {"role": "user", "content": "Introduce yourself."}
  ],
  temperature=0.7,
)

print(completion.choices[0].message)
```

**åœ¨çµ‚ç«¯æ©Ÿèˆ‡ AI é€²è¡Œå°è©±**
```python
# Chat with an intelligent assistant in your terminal
from openai import OpenAI

# Point to the local server
client = OpenAI(base_url="http://localhost:1234/v1", api_key="lm-studio")

history = [
    {"role": "system", "content": "You are an intelligent assistant. You always provide well-reasoned answers that are both correct and helpful."},
    {"role": "user", "content": "Hello, introduce yourself to someone opening this program for the first time. Be concise."},
]

while True:
    completion = client.chat.completions.create(
        model="TheBloke/CodeLlama-7B-Instruct-GGUF/codellama-7b-instruct.Q8_0.gguf",
        messages=history,
        temperature=0.7,
        stream=True,
    )

    new_message = {"role": "assistant", "content": ""}
    
    for chunk in completion:
        if chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end="", flush=True)
            new_message["content"] += chunk.choices[0].delta.content

    history.append(new_message)
    
    # # Uncomment to see chat history
    # import json
    # gray_color = "\033[90m"
    # reset_color = "\033[0m"
    # print(f"{gray_color}\n{'-'*20} History dump {'-'*20}\n")
    # print(json.dumps(history, indent=2))
    # print(f"\n{'-'*55}\n{reset_color}")

    print()
    history.append({"role": "user", "content": input("> ")})
```

**è®€å–åœ–ç‰‡ä¸¦é€²è¡Œæè¿°**
```python
# Adapted from OpenAI's Vision example 
from openai import OpenAI
import base64
import requests

# Point to the local server
client = OpenAI(base_url="http://localhost:1234/v1", api_key="lm-studio")

# Ask the user for a path on the filesystem:
path = input("Enter a local filepath to an image: ")

# Read the image and encode it to base64:
base64_image = ""
try:
  image = open(path.replace("'", ""), "rb").read()
  base64_image = base64.b64encode(image).decode("utf-8")
except:
  print("Couldn't read the image. Make sure the path is correct and the file exists.")
  exit()

completion = client.chat.completions.create(
  model="TheBloke/CodeLlama-7B-Instruct-GGUF/codellama-7b-instruct.Q8_0.gguf",
  messages=[
    {
      "role": "system",
      "content": "This is a chat between a user and an assistant. The assistant is helping the user to describe an image.",
    },
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "Whatâ€™s in this image?"},
        {
          "type": "image_url",
          "image_url": {
            "url": f"data:image/jpeg;base64,{base64_image}"
          },
        },
      ],
    }
  ],
  max_tokens=1000,
  stream=True
)

for chunk in completion:
  if chunk.choices[0].delta.content:
    print(chunk.choices[0].delta.content, end="", flush=True)
```

## æ›¿é¸æ–¹æ¡ˆ
- [GPT4All](https://gpt4all.io/index.html)
- [ollama](https://github.com/ollama/ollama)
- [llamafile](https://github.com/Mozilla-Ocho/llamafile)
